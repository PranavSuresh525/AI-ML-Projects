{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx9jrFIFIXFMHe9rauL++5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PranavSuresh525/AI-ML-Projects/blob/main/AI-ML-Projects/Analytical_Finance_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aBQwkq0Bf86"
      },
      "outputs": [],
      "source": [
        "# All the downloads and imports required\n",
        "!pip install -q -U --no-warn-conflicts \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-google-genai \\\n",
        "    langgraph \\\n",
        "    yfinance \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    newsapi-python \\\n",
        "    langchain-text-splitters \\\n",
        "    langchain-chroma\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import operator\n",
        "from datetime import datetime, timedelta\n",
        "from typing import TypedDict, Annotated\n",
        "import yfinance as yf\n",
        "from newsapi import NewsApiClient\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "from google.colab import userdata\n",
        "import traceback\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## the local llm whixh avoids unecessary calls from gemini as there is a API limit\n",
        "local_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", max_new_tokens=256, device_map=\"auto\")\n",
        "local_llm = HuggingFacePipeline(pipeline=local_pipe)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_TOKEN')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemma-3-27b-it\", # mainly chosen as gemini models allow only 20 tokens per day, this allows 14k\n",
        "    temperature=0.2,\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
        ")\n",
        "_ticker_cache={} # stores already extracted tickers"
      ],
      "metadata": {
        "id": "sByPkSlVF_nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mainly uses the gemma model but in case it fails( due to rate limits) it switches to local model\n",
        "def smart_llm_invoke(prompt):\n",
        "  try:\n",
        "      response = llm.invoke([HumanMessage(content=prompt)])\n",
        "      return response.content.strip()\n",
        "  except Exception as e:\n",
        "      print(f\"Error from Gemini: {e}\")\n",
        "      response = local_llm.invoke(prompt)\n",
        "      return response.strip()"
      ],
      "metadata": {
        "id": "1Eri27MgklVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a basic function to get all the info related to a stock\n",
        "def get_stock_price(ticker: str):\n",
        "  try:\n",
        "    stock = yf.Ticker(ticker)\n",
        "    info = stock.info\n",
        "    history = stock.history(period='1y')\n",
        "\n",
        "    if not info or not info.get('symbol') or history.empty:\n",
        "      return {\"error\": f\"No data found or invalid ticker for {ticker}\"}\n",
        "\n",
        "    return{\n",
        "            'ticker': ticker,\n",
        "            'current_price': info.get('currentPrice', 0),\n",
        "            'previous_close': info.get('previousClose', 0),\n",
        "            'day_high': info.get('dayHigh', 0),\n",
        "            'day_low': info.get('dayLow', 0),\n",
        "            'volume': info.get('volume', 0),\n",
        "            'market_cap': info.get('marketCap', 0),\n",
        "            'company_name': info.get('longName', ticker),\n",
        "            'pe_ratio': info.get('trailingPE', 0),\n",
        "            'dividend_yield': info.get('dividendYield', 0),\n",
        "            'target_mean_price': info.get('targetMeanPrice', 0),\n",
        "            'recommendation_key': info.get('recommendationKey', 'N/A'),\n",
        "            '50_day_average': history['Close'].rolling(window=50).mean().iloc[-1] if len(history) >= 50 else 0,\n",
        "            '200_day_average': history['Close'].rolling(window=200).mean().iloc[-1] if len(history) >= 200 else 0,\n",
        "            'price_history': history['Close'].tail(30).to_list()\n",
        "        }\n",
        "  except Exception as e:\n",
        "        return {\"error\": f\"Failed to fetch data for {ticker}: {str(e)}\"}"
      ],
      "metadata": {
        "id": "V19ta-92QLfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function which accesses a pandas data frame and gets the latest stock price of the company-'item' here\n",
        "def get_recent_data(df, items):\n",
        "  if df.empty:\n",
        "    return {}\n",
        "  recent_data={}\n",
        "  for item in items:\n",
        "    try:\n",
        "      if item in df.index:\n",
        "          value = df.loc[item].iloc[0]\n",
        "          recent_data[item] = float(value) if value is not None else 0\n",
        "      else:\n",
        "          recent_data[item] = 0\n",
        "    except:\n",
        "      recent_data[item] = 0\n"
      ],
      "metadata": {
        "id": "Q6rMmwMGSldA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function which basically gets the latest info (listed below) from yahoo finance, uses the period here to know how long to look for\n",
        "def fetch_financial_statements(ticker: str, period: str)->dict:\n",
        "  try:\n",
        "    stock=yf.Ticker(ticker)\n",
        "    if not stock.info or not stock.info.get('symbol'):\n",
        "      return {'error': f'Invalid or no data for ticker {ticker}'}\n",
        "\n",
        "    if period=='quaterly':\n",
        "      balance_sheet=stock.quarterly_balance_sheet\n",
        "      income_statement=stock.quarterly_income_stmt\n",
        "      cash_flow=stock.quarterly_cashflow\n",
        "    else:\n",
        "      balance_sheet=stock.balance_sheet\n",
        "      income_statement=stock.income_stmt\n",
        "      cash_flow=stock.cashflow\n",
        "    return{\n",
        "        'balance_sheet': get_recent_data(balance_sheet, ['Total Cash', 'Total Debt']),\n",
        "        'income_statement': get_recent_data(income_statement, ['Total Revenue', 'Gross Profit']),\n",
        "        'cash_flow': get_recent_data(cash_flow, ['Net Cash Flow']),\n",
        "        'period': period\n",
        "    }\n",
        "  except Exception as e:\n",
        "    return {\"error\": f\"Failed to fetch data for {ticker}: {str(e)}\"}"
      ],
      "metadata": {
        "id": "v8_nylfNUsdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple validate function that cross checks with yf to see of the ticker exist\n",
        "def validate_ticker(potential_ticker: str):\n",
        "  try:\n",
        "    stock = yf.Ticker(potential_ticker)\n",
        "    info = stock.info\n",
        "    if info and 'symbol' in info and info.get('symbol'):\n",
        "      return info['symbol']\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    return None"
      ],
      "metadata": {
        "id": "yEt8fO7LXq2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifies which currency must be used in the response\n",
        "def fetch_currency(ticker: str):\n",
        "  try:\n",
        "      info = yf.Ticker(ticker).info\n",
        "      currency_code = info.get(\"currency\", \"USD\")\n",
        "  except Exception:\n",
        "      currency_code = \"USD\"\n",
        "  SYMBOL_MAP = {\n",
        "      \"USD\": \"$\", \"INR\": \"₹\", \"EUR\": \"€\", \"GBP\": \"£\",\n",
        "      \"JPY\": \"¥\", \"CNY\": \"¥\", \"HKD\": \"HK$\",\n",
        "      \"AUD\": \"A$\", \"CAD\": \"C$\", \"CHF\": \"CHF\",\n",
        "      \"KRW\": \"₩\", \"BRL\": \"R$\", \"ZAR\": \"R\"\n",
        "  }\n",
        "  symbol = SYMBOL_MAP.get(currency_code, currency_code + \" \")\n",
        "  return symbol, currency_code"
      ],
      "metadata": {
        "id": "AiYT26ZaML1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A function that analyses sentiments using llm, however sometimes, llm fails to extract the sentiment, the function relies on a list of words, some of which have multipliers which are used to amplify the sentiments and negations to reduce it\n",
        "def analyze_sentiment(text: str):\n",
        "    if not text or not text.strip():\n",
        "        return 0.0\n",
        "    prompt = f\"\"\"Analyze the sentiment of this financial news text.\n",
        "Return ONLY a number between -1 (very negative) and 1 (very positive).\n",
        "Text:\n",
        "{text}\n",
        "sentiment:\"\"\"\n",
        "    try:\n",
        "        response = smart_llm_invoke(prompt)\n",
        "        score = float(re.findall(r\"-?\\d+\\.?\\d*\", response)[0])\n",
        "        return max(-1.0, min(1.0, score))\n",
        "    except:\n",
        "        pass\n",
        "    positive_words = {\n",
        "        'gain', 'gains', 'rise', 'rises', 'up', 'surge', 'rally', 'jump',\n",
        "        'soar', 'climb', 'boost', 'profit', 'growth', 'strong',\n",
        "        'outperform', 'beat', 'bullish', 'optimistic', 'upgrade', 'buy'\n",
        "    }\n",
        "    negative_words = {\n",
        "        'loss', 'losses', 'down', 'drop', 'fall', 'decline', 'plunge',\n",
        "        'crash', 'tumble', 'slump', 'miss', 'weak', 'bearish',\n",
        "        'pessimistic', 'downgrade', 'sell', 'concern', 'fear'\n",
        "    }\n",
        "    intensifiers = {'very', 'extremely', 'highly', 'significantly', 'sharply'}\n",
        "    negations = {'not', 'no', 'never', \"don't\", \"doesn't\", \"didn't\", \"won't\"}\n",
        "    words = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
        "\n",
        "    score = 0.0\n",
        "    weight = 1.0\n",
        "    for i, word in enumerate(words):\n",
        "        prev = words[max(0, i-2):i]\n",
        "        multiplier = 1.0\n",
        "        if any(p in intensifiers for p in prev):\n",
        "            multiplier = 1.7\n",
        "        negated = any(p in negations for p in prev)\n",
        "        if word in positive_words:\n",
        "            score += (-multiplier if negated else multiplier)\n",
        "        elif word in negative_words:\n",
        "            score += (multiplier if negated else -multiplier)\n",
        "    if score == 0:\n",
        "        return 0.0\n",
        "    normalized = score / max(3.0, abs(score))\n",
        "    return max(-1.0, min(1.0, normalized))"
      ],
      "metadata": {
        "id": "IYpfw7IE6DpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic features that the model must have\n",
        "class AgentState(TypedDict):\n",
        "    query: str\n",
        "    ticker: str\n",
        "    intent: str\n",
        "    price_data: dict\n",
        "    financial_data: dict\n",
        "    news_articles: Annotated[list, operator.add]\n",
        "    news_context: Annotated[list, operator.add]\n",
        "    sentiment_score: float\n",
        "    analysis: str\n",
        "    recommendation: str\n",
        "    messages: list\n",
        "    news_sources: list"
      ],
      "metadata": {
        "id": "03-EXbKtcsyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The main RAG pipeline\n",
        "class NewsRAG:\n",
        "    def __init__(self):\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        ) # the model chosen to embed the text into a vector space\n",
        "        self.vectorstore = None\n",
        "\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=100\n",
        "        ) # basically cuts the text into chunks of 500 characters each of which 100 overlap with the previous one\n",
        "    def index_news(self, articles: list): # used to store the info goven below\n",
        "        documents = []\n",
        "        for article in articles:\n",
        "            text = f\"\"\"\n",
        "            Title: {article['title']}\n",
        "            Source: {article['source']}\n",
        "            Date: {article['date']}\n",
        "            Content: {article['content']}\n",
        "            \"\"\"\n",
        "            docs = self.splitter.split_documents([\n",
        "                Document(\n",
        "                    page_content=text,\n",
        "                    metadata={\n",
        "                        \"source\": article[\"source\"],\n",
        "                        \"url\": article[\"link\"],\n",
        "                        \"date\": article[\"date\"]\n",
        "                    }\n",
        "                )\n",
        "            ])\n",
        "            documents.extend(docs)\n",
        "        if documents: # vectors are stored in chroma\n",
        "            self.vectorstore = Chroma.from_documents(\n",
        "                documents,\n",
        "                self.embeddings,\n",
        "                collection_name=\"news_rag\"\n",
        "            )\n",
        "    def retrieve_context(self, query: str, k: int = 5): # used to retrieve the vectors from the embedded space\n",
        "        if not self.vectorstore:\n",
        "            return []\n",
        "        results = self.vectorstore.similarity_search(query, k=k)\n",
        "        return [\n",
        "            {\n",
        "                \"content\": doc.page_content,\n",
        "                \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n",
        "                \"url\": doc.metadata.get(\"url\", \"\"),\n",
        "                \"date\": doc.metadata.get(\"date\", \"\")\n",
        "            }\n",
        "            for doc in results\n",
        "        ]"
      ],
      "metadata": {
        "id": "AZvakxK2WjcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a helper function which calls the RAG setup cleanly, here we take k=5 so we call the top 5 chunks which are closest to the query mathematically\n",
        "def rag_retriever(state: AgentState):\n",
        "  articles = state.get(\"news_articles\", [])\n",
        "  if not articles:\n",
        "      state[\"news_context\"] = []\n",
        "      state[\"news_sources\"] = []\n",
        "      return state\n",
        "  rag = NewsRAG()\n",
        "  rag.index_news(articles)\n",
        "  retrieved_dicts = rag.retrieve_context(state[\"query\"], k=5)\n",
        "  state[\"news_context\"] = [r[\"content\"] for r in retrieved_dicts]\n",
        "  state[\"news_sources\"] = retrieved_dicts\n",
        "  return state"
      ],
      "metadata": {
        "id": "MVlmzM8tW2Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifies the query into the following categories to allow a more precise answer\n",
        "def intent_classifier(state: AgentState):\n",
        "    query = state['query']\n",
        "    query_lower = query.lower()\n",
        "    if any(word in query_lower for word in ['when did', 'when was']) and any(word in query_lower for word in ['hit', 'reach', 'achieve', 'cross']):\n",
        "        intent = 'milestone_query'\n",
        "    elif any(word in query_lower for word in ['why', 'reason', 'cause']):\n",
        "        intent = 'reason_query'\n",
        "    elif any(word in query_lower for word in ['when', 'trend', 'history']):\n",
        "        intent = 'trend_analysis'\n",
        "    elif any(word in query_lower for word in ['compare', 'vs', 'versus']):\n",
        "        intent = 'comparison'\n",
        "    elif any(word in query_lower for word in ['price', 'cost', 'trading at']):\n",
        "        intent = 'price_query'\n",
        "    else:\n",
        "        intent = 'general'\n",
        "    state['intent'] = intent\n",
        "    state['messages'].append(f\"[Intent Classifier] Intent: {intent}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "-2CCz_-_gIgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function that uses yahoo api to search across yahoo's database to get the exact ticker that the query want so give\n",
        "def yahoo_lookup_best_match(search_term):# searches through yahoo's database\n",
        "  try:\n",
        "    url = \"https://query2.finance.yahoo.com/v1/finance/search\"\n",
        "    params = {\n",
        "        'q': search_term,\n",
        "        'quotesCount': 10,\n",
        "        'newsCount': 0,\n",
        "        'enableFuzzyQuery': False,\n",
        "        'quotesQueryId': 'tss_match_phrase_query'\n",
        "    }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, params=params, headers=headers, timeout=5)\n",
        "    data = response.json()\n",
        "    quotes = data.get('quotes', [])\n",
        "    if not quotes:\n",
        "        return None\n",
        "# the problem with having just the company's name is that it might pickup on any subsidary or minor company with the same name ( due to alphabetical order of yahoo database) thus assigns a ranking system to the search results\n",
        "    def score_result(quote):\n",
        "      score = 0\n",
        "      symbol = quote.get('symbol', '')\n",
        "      name = quote.get('longname', '') or quote.get('shortname', '')\n",
        "      exchange = quote.get('exchange', '')\n",
        "      quote_type = quote.get('quoteType', '')\n",
        "      if search_term.lower() in name.lower():\n",
        "          score += 100\n",
        "      if quote_type == 'EQUITY':\n",
        "          score += 50\n",
        "      if exchange in ['NSI', 'BSE', 'NMS', 'NYQ']:\n",
        "          score += 30\n",
        "      if len(name) > len(search_term) + 20:\n",
        "          score -= 20\n",
        "      if len(symbol) <= 10:\n",
        "          score += 10\n",
        "      base_symbol = symbol.split('.')[0]\n",
        "      if base_symbol.isalpha():\n",
        "          score += 10\n",
        "      return score\n",
        "\n",
        "    ranked = sorted(quotes, key=score_result, reverse=True)\n",
        "    return ranked[0].get('symbol')\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Yahoo lookup failed: {e}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "fJ0w1urkDvm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the bottleneck of this project, the following function has a 3 stage fallback option to try to extract a ticker at all costs\n",
        "def ticker_extractor(state):\n",
        "  # uses regex search first, in case the query contains the ticker directly, this if found is extremely fast\n",
        "    query = state[\"query\"].strip()\n",
        "    q_upper = query.upper()\n",
        "    explicit_ticker_pattern = r'\\b[A-Z&]{2,10}\\.[A-Z]{1,4}\\b|\\$[A-Z]{2,5}\\b'\n",
        "    explicit_matches = re.findall(explicit_ticker_pattern, q_upper)\n",
        "    for raw in explicit_matches: # checks the find with validate ticker function\n",
        "        ticker = raw.replace(\"$\", \"\").strip()\n",
        "        if validate_ticker(ticker):\n",
        "            state[\"ticker\"] = ticker\n",
        "            state[\"messages\"].append(f\"[Ticker Extractor] Explicit ticker: {ticker}\")\n",
        "            return state\n",
        "# if there is no direct ticker in the query, it proceeds to use a llm to extract the company name\n",
        "    extract_prompt = f\"\"\"Extract ONLY the company name from this query. Return just the company name, nothing else.\n",
        "Query: {query}\n",
        "Company name:\"\"\"\n",
        "    company_name = smart_llm_invoke(extract_prompt).strip()\n",
        "# calls upon the yahoo function from earlier\n",
        "    ticker = yahoo_lookup_best_match(company_name)\n",
        "    if ticker and validate_ticker(ticker):\n",
        "        state[\"ticker\"] = ticker\n",
        "        state[\"messages\"].append(f\"[Ticker Extractor] Found via Yahoo: {ticker}\")\n",
        "        return state\n",
        "# if the above 2 methods fail, it uses the last method, i.e tries to extract the ticker using the following prompt\n",
        "    llm_prompt = f\"\"\"What is the PRIMARY/MAIN stock ticker symbol for \"{company_name}\"?\n",
        "IMPORTANT: If there are multiple companies with similar names, return the LARGEST/MOST WELL-KNOWN one.\n",
        "Examples:\n",
        "- \"Mahindra\" → M&M.NS (Mahindra & Mahindra, the main automobile company, NOT subsidiaries)\n",
        "- \"Tata\" → TATAMOTORS.NS (the main auto company)\n",
        "- \"Adani\" → ADANI.NS (Adani Enterprises, the flagship)\n",
        "- \"Apple\" → AAPL (the main company)\n",
        "For Indian companies: use .NS suffix\n",
        "For US companies: no suffix\n",
        "Company: {company_name}\n",
        "Main ticker symbol:\"\"\"\n",
        "    ticker_response = smart_llm_invoke(llm_prompt).strip().upper()\n",
        "# tries to regex search the results from the llm as it should output the ticker\n",
        "    match = re.search(r'\\b[A-Z&]{1,15}\\.(?:NS|BO)\\b|\\b[A-Z]{1,5}\\b', ticker_response)\n",
        "    ticker = match.group(0) if match else ticker_response\n",
        "    if ticker and validate_ticker(ticker):\n",
        "        state[\"ticker\"] = ticker\n",
        "        state[\"messages\"].append(f\"[Ticker Extractor] LLM resolved: {ticker}\")\n",
        "        return state\n",
        "    state[\"ticker\"] = \"\"\n",
        "    state[\"messages\"].append(f\"[Ticker Extractor] Could not find ticker for {company_name}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "_23gH8FVAPfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uses the get stock info function to update the agent's parameters\n",
        "def price_fetcher(state: AgentState):\n",
        "  ticker=state['ticker']\n",
        "  if ticker:\n",
        "    price_data=get_stock_price(ticker)\n",
        "    state['price_data']=price_data\n",
        "    state['messages'].append(f\"[Price Fetcher] Price Data: {price_data}\")\n",
        "  else:\n",
        "    state['price_data']={}\n",
        "    state['messages'].append(f\"[Price Fetcher] No price data found\")\n",
        "  return state"
      ],
      "metadata": {
        "id": "cfX7ygAEcyub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feches financial info if the intent classifies the query as general or comparision\n",
        "def financial_fetcher(state: AgentState):\n",
        "  ticker=state['ticker']\n",
        "  intent=state['intent']\n",
        "  if intent in ['general', 'comparison'] and ticker:\n",
        "    financial_data=fetch_financial_statements(ticker, 'annual')\n",
        "    state['financial_data']=financial_data\n",
        "    state['messages'].append(f\"[Financial Fetcher] Financial Data: {financial_data}\")\n",
        "  else:\n",
        "    state['financial_data']={}\n",
        "    state['messages'].append(f\"[Financial Fetcher] No financial data found\")\n",
        "  return state"
      ],
      "metadata": {
        "id": "A6JAhSvddVNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a crucial function in this setup, fetches the news that the RAG will embed, i used a news api to get the source\n",
        "def news_fetcher(state: AgentState):\n",
        "    ticker = state.get('ticker')\n",
        "    if not ticker:\n",
        "        state['news_articles'] = []\n",
        "        state.setdefault('messages', []).append(\"[News Fetcher] No ticker provided\")\n",
        "        return state\n",
        "    try:\n",
        "        try:\n",
        "            api_key = userdata.get('NEWSAPI_KEY')\n",
        "            newsapi = NewsApiClient(api_key=api_key)\n",
        "        except Exception as e:\n",
        "            state['news_articles'] = []\n",
        "            state.setdefault('messages', []).append(f\"[News Fetcher] API initialization failed: {e}\")\n",
        "            return state\n",
        "# reduces the ticker to its company name or get the company name from price_data function\n",
        "        clean_ticker = ticker.split('.')[0]\n",
        "        price_data = state.get('price_data', {})\n",
        "        company_name = price_data.get('company_name', clean_ticker)\n",
        "        intent = state.get('intent', '')\n",
        "        from_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "\n",
        "        articles = []\n",
        "        seen_urls = set()\n",
        "        queries = [\n",
        "            company_name,\n",
        "            f\"{clean_ticker}\",\n",
        "            f\"{company_name} stock\",\n",
        "            f\"{company_name} India\",\n",
        "        ]\n",
        "        for query in queries:\n",
        "            try: # proceeds to search and gets the site name with its URL along with its date\n",
        "                response = newsapi.get_everything(\n",
        "                    q=query,\n",
        "                    from_param=from_date,\n",
        "                    language='en',\n",
        "                    sort_by='publishedAt',\n",
        "                    page_size=20\n",
        "                )\n",
        "                status = response.get('status')\n",
        "                news_items = response.get('articles', [])\n",
        "                if status != 'ok':\n",
        "                    continue\n",
        "                for item in news_items:\n",
        "                    url = item.get('url', '')\n",
        "                    if not url or url in seen_urls:\n",
        "                        continue\n",
        "                    seen_urls.add(url)\n",
        "                    source_obj = item.get('source', {})\n",
        "                    source_name = source_obj.get('name', '') if isinstance(source_obj, dict) else str(source_obj)\n",
        "                    content = item.get('description') or item.get('content') or item.get('title', '')\n",
        "                    published_date = item.get('publishedAt', '')\n",
        "                    try:\n",
        "                        date_obj = datetime.fromisoformat(published_date.replace('Z', '+00:00'))\n",
        "                        formatted_date = date_obj.strftime('%Y-%m-%d')\n",
        "                    except:\n",
        "                        formatted_date = published_date\n",
        "\n",
        "                    articles.append({\n",
        "                        'title': item.get('title', ''),\n",
        "                        'content': content,\n",
        "                        'link': url,\n",
        "                        'date': formatted_date,\n",
        "                        'source': source_name\n",
        "                    })\n",
        "                if len(articles) >= 20:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        state['news_articles'] = articles\n",
        "        state.setdefault('messages', [])\n",
        "        state['messages'].append(f\"[News Fetcher] Found {len(articles)} articles for {ticker}\")\n",
        "    except Exception as e:\n",
        "        state['news_articles'] = []\n",
        "        state.setdefault('messages', []).append(f\"[News Fetcher] Error: {str(e)}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "umQuooOqd3ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that updates the state class after the news fetcher is done\n",
        "def news_analyzer(state: AgentState):\n",
        "    context = state.get(\"news_context\", [])\n",
        "    if not context:\n",
        "        state[\"sentiment_score\"] = 0.0\n",
        "        state[\"messages\"].append(\"[News Analyzer] No context\")\n",
        "        return state\n",
        "    sentiment = analyze_sentiment(\"\\n\".join(context))\n",
        "    state[\"sentiment_score\"] = sentiment\n",
        "    state[\"messages\"].append(\n",
        "        f\"[News Analyzer] Sentiment Score: {sentiment:.2f}\"\n",
        "    )\n",
        "    return state"
      ],
      "metadata": {
        "id": "LE8QITsWfGss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the response generation node, takes all the parameters of the agent and asks the llm to stich together a coherant answer using the context from the information\n",
        "def response_generator(state: AgentState):\n",
        "  query = state[\"query\"]\n",
        "  ticker = state[\"ticker\"]\n",
        "  price_data = state.get(\"price_data\", {})\n",
        "  sentiment = state.get(\"sentiment_score\", 0.0)\n",
        "  news_context = state.get(\"news_context\") or []\n",
        "  sources = state.get(\"news_sources\") or []\n",
        "  symbol = fetch_currency(ticker)[0]\n",
        "  news_str = \"\\n\".join(news_context) if news_context else \"No specific news found.\"\n",
        "  prompt = f\"\"\"\n",
        "  You are a professional financial analyst.\n",
        "  STEP 1: ANALYZE PRICE DATA\n",
        "  Ticker: {ticker}\n",
        "  Current Price: {symbol}{price_data.get('current_price')}\n",
        "  Change: From {symbol}{price_data.get('previous_close')}\n",
        "  STEP 2: INCORPORATE RAG CONTEXT (News)\n",
        "  Recent News Context: {news_str}\n",
        "  Sentiment Score: {sentiment:.2f}\n",
        "  USER QUESTION: {query}\n",
        "  INSTRUCTIONS:\n",
        "  1. Provide a \"Market Snapshot\" based on the price data.\n",
        "  2. Provide a \"News & Catalyst\" section using the RAG context.\n",
        "  3. Be factual and grounded in the provided snippets.\n",
        "  \"\"\"\n",
        "  answer = smart_llm_invoke(prompt)\n",
        "# adds the news sources that the news fetcher accessed during its search to the final answer\n",
        "  if sources:\n",
        "      answer += \"\\n\\n---\"\n",
        "      answer += \"\\n### Latest News & Sources:\"\n",
        "      seen_urls = set()\n",
        "      for i, s in enumerate(sources, 1):\n",
        "          url = s.get('url')\n",
        "          if url and url not in seen_urls:\n",
        "              seen_urls.add(url)\n",
        "              answer += f\"\\n{i}. {s.get('source', 'News')} – [Read Article]({url})\"\n",
        "  state[\"analysis\"] = answer\n",
        "  return state"
      ],
      "metadata": {
        "id": "BeNaONHsfoDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the function which puts the entire flow together and defines the order of callig the functions and the pipeline that has to be run\n",
        "def build_workflow():\n",
        "  workflow = StateGraph(AgentState)\n",
        "  # Nodes\n",
        "  workflow.add_node(\"intent_classifier\", intent_classifier)\n",
        "  workflow.add_node(\"ticker_extractor\", ticker_extractor)\n",
        "  workflow.add_node(\"price_fetcher\", price_fetcher)\n",
        "  workflow.add_node(\"financial_fetcher\", financial_fetcher)\n",
        "  workflow.add_node(\"news_fetcher\", news_fetcher)\n",
        "  workflow.add_node(\"news_analyzer\", news_analyzer)\n",
        "  workflow.add_node(\"rag_retriever\", rag_retriever)\n",
        "  workflow.add_node(\"response_generator\", response_generator)\n",
        "  # Linear Execution: Force it to always gather News for the RAG display\n",
        "  workflow.add_edge(START, \"intent_classifier\")\n",
        "  workflow.add_edge(\"intent_classifier\", \"ticker_extractor\")\n",
        "  workflow.add_edge(\"ticker_extractor\", \"price_fetcher\")\n",
        "  workflow.add_edge(\"price_fetcher\", \"financial_fetcher\")\n",
        "  workflow.add_edge(\"financial_fetcher\", \"news_fetcher\")\n",
        "  workflow.add_edge(\"news_fetcher\", \"news_analyzer\")\n",
        "  workflow.add_edge(\"news_analyzer\", \"rag_retriever\")\n",
        "  workflow.add_edge(\"rag_retriever\", \"response_generator\")\n",
        "  workflow.add_edge(\"response_generator\", END)\n",
        "\n",
        "  return workflow.compile()"
      ],
      "metadata": {
        "id": "Yx1lErg_gZS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function that helps run the chatbot, it initializes all values of the agent, calls the agent and feeds it the user's query\n",
        "def query_stocks(user_query: str):\n",
        "    initial_state = {\n",
        "        'query': user_query,\n",
        "        'ticker': '',\n",
        "        'intent': '',\n",
        "        'price_data': {},\n",
        "        'financial_data': {},\n",
        "        'news_articles': [],\n",
        "        'news_context': [],\n",
        "        'sentiment_score': 0.0,\n",
        "        'analysis': '',\n",
        "        'recommendation': '',\n",
        "        'messages': []\n",
        "    }\n",
        "    agent = build_workflow()\n",
        "    result = agent.invoke(initial_state)\n",
        "    return {\n",
        "        'answer': result['analysis'],\n",
        "        'ticker': result['ticker'],\n",
        "        'sentiment': result['sentiment_score'],\n",
        "        'debug_messages': result['messages']\n",
        "    }"
      ],
      "metadata": {
        "id": "97yTJnkShTzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fb107d",
        "outputId": "810acd4a-a983-41df-e0f5-50d2dd3af874"
      },
      "source": [
        "query=\"Why did Apple stock drop?\"\n",
        "result2 = query_stocks(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Answer: {result2['answer']}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Why did Apple stock drop?\n",
            "Answer: ## Apple (AAPL) Stock Analysis - January 26, 2026\n",
            "\n",
            "Here's an analysis of Apple's stock performance, incorporating available price data and recent news:\n",
            "\n",
            "**1. Market Snapshot**\n",
            "\n",
            "* **Ticker:** AAPL\n",
            "* **Current Price:** $254.8899\n",
            "* **Change:** +$6.8499 (from $248.04)\n",
            "* **Direction:** Positive - Apple stock *increased* in price, it did *not* drop. The provided data shows a gain, not a loss.\n",
            "\n",
            "**2. News & Catalyst**\n",
            "\n",
            "The recent news context primarily focuses on historical documentation related to Apple's founding. Specifically, the original partnership agreement between Steve Jobs, Steve Wozniak, and Ronald Wayne sold for $2.51 million on January 23, 2026. This news, while interesting from a historical perspective, **does not provide any explanation for stock movement.** It's unlikely to be a catalyst for either a price increase or decrease. \n",
            "\n",
            "There is also a news item regarding Alphabet (GOOGL) and its advancements in AI and cloud, highlighted by Bill Ackman's positive outlook. However, this news pertains to a *competitor* and doesn't directly explain any movement in Apple's stock price.\n",
            "\n",
            "**Regarding the user's question (\"Why did Apple stock drop?\"):** Based on the provided data, Apple stock did *not* drop. It *increased* by $6.8499. The provided news snippets do not offer any insight into a potential drop in price. Further data and news sources would be needed to understand any potential negative pressures on the stock.\n",
            "\n",
            "\n",
            "\n",
            "**Disclaimer:** I am an AI chatbot and cannot provide financial advice. This analysis is based solely on the limited data provided and should not be used as a basis for investment decisions.\n",
            "\n",
            "---\n",
            "### Latest News & Sources:\n",
            "1. Cllct.com – [Read Article](https://www.cllct.com/sports-collectibles/auctions/original-apple-computer-document-sells-for-2-51-million-at-christie-s)\n",
            "5. Yahoo Entertainment – [Read Article](https://finance.yahoo.com/news/alphabet-inc-googl-strengthens-position-044216933.html)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}